# Non-Blocking Architecture Implementation - COMPLETE

**Date:** 2026-01-29
**Objective:** Enable non-blocking NetworkManager architecture with safe ring gating for 10GB/s throughput

---

## Implementation Status: âœ… COMPLETE

All planned changes have been successfully implemented and the project builds without errors.

---

## Changes Made

### 1. Added Non-Blocking Ring Gating (src/embarlet/topic.cc)

**Location:** Lines 1146-1193 (EmbarcaderoGetCXLBuffer function)

**What Changed:**
- Added safety check before allocating each ring slot
- Reads `batch_headers_consumed_through` from sequencer with cache invalidation
- Returns `nullptr` immediately if slot not free (fail-fast, non-blocking)
- Includes detailed logging for ring-full events

**Key Features:**
```cpp
// Check if slot is free
size_t next_slot_offset = static_cast<size_t>(batch_headers_ - batch_headers_start);
const void* consumed_through_addr = const_cast<const void*>(
    reinterpret_cast<const volatile void*>(&tinode_->offsets[broker_id_].batch_headers_consumed_through));
CXL::flush_cacheline(consumed_through_addr);
CXL::load_fence();
size_t consumed_through = tinode_->offsets[broker_id_].batch_headers_consumed_through;

bool slot_free = (consumed_through == BATCHHEADERS_SIZE) ||
                 (consumed_through >= next_slot_offset + sizeof(BatchHeader));

if (!slot_free) {
    log = nullptr;
    batch_header_location = nullptr;
    return nullptr;  // Fail-fast
}
```

**Benefits:**
- Prevents silent overwrite of unconsumed batches
- No blocking â†’ no TCP timeouts
- Works with wrap-around correctly
- Minimal overhead (one CXL read under mutex)

---

### 2. Enabled Non-Blocking Mode by Default (src/common/configuration.h)

**Location:** Lines 88-93

**What Changed:**
```cpp
// Before:
ConfigValue<bool> use_nonblocking{false, "EMBARCADERO_USE_NONBLOCKING"};
ConfigValue<int> staging_pool_num_buffers{32, "EMBARCADERO_STAGING_POOL_NUM_BUFFERS"};
ConfigValue<int> num_publish_receive_threads{4, "EMBARCADERO_NUM_PUBLISH_RECEIVE_THREADS"};
ConfigValue<int> num_cxl_allocation_workers{2, "EMBARCADERO_NUM_CXL_ALLOCATION_WORKERS"};

// After:
ConfigValue<bool> use_nonblocking{true, "EMBARCADERO_USE_NONBLOCKING"};
ConfigValue<int> staging_pool_num_buffers{128, "EMBARCADERO_STAGING_POOL_NUM_BUFFERS"};
ConfigValue<int> num_publish_receive_threads{8, "EMBARCADERO_NUM_PUBLISH_RECEIVE_THREADS"};
ConfigValue<int> num_cxl_allocation_workers{4, "EMBARCADERO_NUM_CXL_ALLOCATION_WORKERS"};
```

**Benefits:**
- Non-blocking mode active by default (can override with env var)
- Staging pool increased to 256MB (128 Ã— 2MB buffers)
- Receive threads increased to 8 (1 per 2 publishers)
- CXL workers increased to 4 (match receive threads)

---

### 3. Added Ring Full Metric (src/network_manager/)

**Location:**
- Header: network_manager.h:213
- Implementation: network_manager.cc:2063, 2066-2072

**What Changed:**
- Added `std::atomic<uint64_t> metric_ring_full_{0}` to NetworkManager class
- Increment metric in CXLAllocationWorker when GetCXLBuffer returns nullptr
- Enhanced logging to show total ring-full events

**Benefits:**
- Visibility into how often ring fills
- Helps diagnose sequencer performance issues
- Tracks retry behavior

---

### 4. Updated YAML Configuration (config/embarcadero.yaml)

**Location:** Lines 48-57 (new section added)

**What Changed:**
Added new network configuration section:
```yaml
network:
  # Non-blocking I/O architecture (ENABLED for 10GB/s throughput target)
  use_nonblocking: true        # Enable non-blocking NetworkManager architecture
  staging_pool_buffer_size_mb: 2   # Size of each staging buffer (2MB matches batch size)
  staging_pool_num_buffers: 128    # Number of staging buffers (256MB total for 10GB pipeline)
  num_publish_receive_threads: 8   # Epoll threads for socket draining (1 per 2 publishers)
  num_cxl_allocation_workers: 4    # Async CXL allocation workers (match receive threads)
```

**Benefits:**
- Configuration is self-documenting
- Easy to tune for different workloads
- Can override with environment variables

---

## Files Modified

| File | Lines Changed | Risk Level |
|------|---------------|------------|
| src/embarlet/topic.cc | ~50 lines | Medium (core allocation path) |
| src/common/configuration.h | ~15 lines | Low (config only) |
| src/network_manager/network_manager.h | 1 line | Low (declaration) |
| src/network_manager/network_manager.cc | ~10 lines | Low (logging) |
| config/embarcadero.yaml | ~10 lines | Low (config) |

**Total:** ~86 lines of code changed

---

## Architecture Overview

### Before (Blocking Mode)
```
Publisher â†’ TCP â†’ NetworkManager Thread
                    â†“ [recv() BLOCKS]
                    â†“ [mutex on GetCXLBuffer]
                    â†“ [Other threads wait on mutex]
                    â†“ [TCP buffers overflow]
                    â†“ [Connection timeout after ~10 batches]
```

**Problem:** 16 publishers Ã— blocking recv + mutex contention = TCP timeouts at ~37.6% completion

### After (Non-Blocking Mode)
```
Publisher â†’ TCP â†’ PublishReceiveThread (epoll)
                    â†“ [non-blocking recv to StagingPool]
                    â†“ [no mutex contention]
                    â†“
                  cxl_allocation_queue (lock-free MPMC)
                    â†“
                  CXLAllocationWorker
                    â†“ [GetCXLBuffer with ring gating]
                    â†“ [retry if ring full]
                    â†“ [memcpy + flush]
                    â†“ [batch_complete=1]
```

**Benefits:**
- Socket draining decoupled from CXL allocation
- No mutex contention on receive path
- Lock-free queues for communication
- Exponential backoff on ring-full (no TCP timeout)

---

## Testing Plan

### Phase 1: Validation (Immediate)

1. **Verify non-blocking mode is active:**
   ```bash
   grep "PublishReceiveThread started" /tmp/embarcadero*.log
   grep "CXLAllocationWorker started" /tmp/embarcadero*.log
   ```

2. **Run 1GB test:**
   ```bash
   ./scripts/run_throughput.sh 1GB 1024
   ```
   **Expected:** 100% ACK completion, >500 MB/s

3. **Run 10GB test:**
   ```bash
   ./scripts/run_throughput.sh 10GB 1024
   ```
   **Expected:** Significant improvement over 37.6%, ideally 100% ACK

### Phase 2: Metrics Analysis

Check for ring-full events:
```bash
grep "Ring full" /tmp/embarcadero*.log
grep "metric_ring_full" /tmp/embarcadero*.log
```

**If ring fills frequently:**
- Sequencer may be slow â†’ profile BrokerScannerWorker5
- Ring may need to be larger â†’ increase BATCHHEADERS_SIZE
- Too many publishers â†’ reduce concurrency

**If ring never fills:**
- System is healthy
- Staging pool is absorbing bursts correctly
- Sequencer is keeping up

### Phase 3: Performance Benchmarking

| Test | Baseline (Blocking) | Target (Non-Blocking) | Actual |
|------|---------------------|----------------------|--------|
| 1GB Throughput | 456 MB/s | 1-2 GB/s | TBD |
| 1GB ACK % | 100% | 100% | TBD |
| 10GB Throughput | 46 MB/s (stall) | 1+ GB/s | TBD |
| 10GB ACK % | 37.6% | 100% | TBD |

---

## Rollback Plan

If issues are encountered:

### Option 1: Disable non-blocking mode via environment variable
```bash
export EMBARCADERO_USE_NONBLOCKING=0
```

### Option 2: Revert configuration changes
```bash
git checkout src/common/configuration.h
git checkout config/embarcadero.yaml
cmake --build build
```

### Option 3: Full revert
```bash
git checkout src/embarlet/topic.cc
git checkout src/common/configuration.h
git checkout src/network_manager/network_manager.h
git checkout src/network_manager/network_manager.cc
git checkout config/embarcadero.yaml
cmake --build build
```

---

## Next Steps

1. **Run validation tests** (1GB and 10GB)
2. **Analyze metrics** (ring_full, staging_exhausted, cxl_retries)
3. **Profile if needed** (sequencer, CXLAllocationWorker)
4. **Tune configuration** based on results:
   - Increase staging pool if exhaustion occurs
   - Increase workers if CXL allocation lags
   - Adjust ring size if wrapping issues
5. **Stress testing** (multiple runs, different sizes)
6. **Production hardening** based on findings

---

## Key Design Decisions

### Why Non-Blocking Instead of Blocking?

**Blocking mode fails because:**
- Mutex contention on GetCXLBuffer (16 publishers, 1 mutex)
- Thread blocks while holding socket â†’ TCP buffers overflow
- Connections timeout after ~10 batches
- Only ~40 batches received per broker before failure

**Non-blocking mode succeeds because:**
- Socket draining uses non-blocking recv (MSG_DONTWAIT)
- Epoll with edge-triggering for efficiency
- Staging pool absorbs bursts without CXL mutex
- CXL allocation happens async in separate workers
- Retry with exponential backoff on ring-full

### Why Fail-Fast Ring Gating?

**Blocking ring gating fails because:**
- Producer blocks waiting for sequencer
- While blocked, can't drain TCP â†’ connections timeout
- Same failure mode as blocking recv

**Fail-fast ring gating succeeds because:**
- Return nullptr immediately when ring full
- Caller (CXLAllocationWorker) retries with backoff
- Socket draining continues unaffected
- No TCP timeouts

### Why These Configuration Values?

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| staging_pool_num_buffers | 128 | 256MB total = 128 batches of 2MB each, provides headroom for burst traffic |
| num_publish_receive_threads | 8 | 16 publishers / 2 = 8 threads, good multiplexing ratio |
| num_cxl_allocation_workers | 4 | Match receive threads, each worker handles ~2 threads' worth of batches |
| batch_headers_size | 10MB | 163,840 slots >> 1,360 needed for 10GB, plenty of headroom |

---

## Code References

| Component | File | Lines |
|-----------|------|-------|
| Non-blocking ring gating | topic.cc | 1146-1193 |
| Configuration defaults | configuration.h | 88-98 |
| Ring full metric | network_manager.h | 213 |
| Ring full logging | network_manager.cc | 2063, 2066-2072 |
| YAML config | embarcadero.yaml | 48-57 |
| PublishReceiveThread | network_manager.cc | 1831-2023 |
| CXLAllocationWorker | network_manager.cc | 2029-2146 |
| StagingPool | staging_pool.h/cc | - |

---

## Success Criteria

### Minimum (Phase 1)
- âœ… Code compiles without errors
- [ ] Non-blocking mode activates (check logs)
- [ ] 1GB test completes with 100% ACK
- [ ] 10GB test improves beyond 37.6%

### Target (Phase 2)
- [ ] 1GB test: 1-2 GB/s throughput
- [ ] 10GB test: 100% ACK completion
- [ ] 10GB test: 1+ GB/s throughput
- [ ] No TCP connection timeouts

### Stretch (Phase 3)
- [ ] 10GB test: 5-7 GB/s throughput
- [ ] p99 latency: <1ms
- [ ] Ring full events: <100 total
- [ ] Zero batch drops

---

## Conclusion

The implementation is complete and the code builds successfully. The key insight is that **a sophisticated non-blocking architecture already existed in the codebase** but was disabled. We've activated it, added safe ring gating, and tuned configuration for high throughput.

The next critical step is **validation testing** to measure actual throughput improvement and verify that the 37.6% stall is resolved.

**Ready for testing! ðŸš€**
